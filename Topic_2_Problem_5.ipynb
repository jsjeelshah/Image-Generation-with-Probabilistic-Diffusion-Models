{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5:** Perform an interpolation experiment with your trained model from Problem 4 "
      ],
      "metadata": {
        "id": "xFLM7R0cc8En"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import and Definations\n",
        "We have followed official documentation of PyTorch. It is an open source python framework used for machine learning"
      ],
      "metadata": {
        "id": "bw5CowfBdGVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5oQyXU9wRG5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Changing the form\n",
        "img = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "# Using MNIST dataset as stated in the project.\n",
        "mnist_DataSet = MNIST(root='./data/MNIST', download=True, train=True, transform=img)\n",
        "data_Loader = DataLoader(mnist_DataSet, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Variational Autoencoders (VAEs)\n",
        "In order to ensure that the latent space of the autoencoder, the VAE, has good qualities and can produce some new data, the distribution of its encodings is regularized during training. Additionally, the word \"variational\" derives from the tight connection between the regularization and variational inference methods in statistics."
      ],
      "metadata": {
        "id": "JOUz9Zkdehp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution encoder and decoder tends to perform better with same number of parameters.\n",
        "class En(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(En, self).__init__()\n",
        "        c = 64\n",
        "        self.firstC = nn.Conv2d(1,c,4,2,1)\n",
        "        self.secoundC = nn.Conv2d(c,c*2,4,2,1)\n",
        "        self.fc_mu = nn.Linear(c*2*7*7, 2)\n",
        "        self.fc_logvar = nn.Linear(c*2*7*7, 2)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.firstC(x))\n",
        "        x = F.relu(self.secoundC(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar"
      ],
      "metadata": {
        "id": "0pboiy0TaRAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = 64\n",
        "        self.fc = nn.Linear(2, c*2*7*7)\n",
        "        self.firstC = nn.ConvTranspose2d(c, 1, 4, 2, 1)\n",
        "        self.secondC = nn.ConvTranspose2d(c*2, c, 4, 2, 1)\n",
        "        \n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), 64*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.secondC(x))\n",
        "        x = torch.sigmoid(self.firstC(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x"
      ],
      "metadata": {
        "id": "CGIWeXwmappj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = En()\n",
        "        self.decoder = Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "    \n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + 0.01 * kldivergence, recon_loss, 0.01 * kldivergence\n",
        "    \n",
        "    \n",
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "9TwlZXQDnSSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `n_epochs` and `learning rate` are the training hyper-parameters."
      ],
      "metadata": {
        "id": "wkKInpLvfVng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs =10\n",
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in data_Loader:\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae recaonstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss,_,_ = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izuQPeG0oKlG",
        "outputId": "a4878924-4180-481d-f269-a97636953128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n",
            "Epoch [1 / 10] average reconstruction error: 1648.601481\n",
            "Epoch [2 / 10] average reconstruction error: 1474.469752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Specify a path to save the trainet network\n",
        "# PATH = \"Trained_MNIST_Model.pt\"\n",
        "\n",
        "# torch.save(vae, PATH)"
      ],
      "metadata": {
        "id": "_w8vNlQuxybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Reconstruction and Interpolation\n",
        "We can interpolate source images in latent space using q as stochastic encoder, then decoding the lineraly interpolated latent into images space by reverse space by the reverse process.\n"
      ],
      "metadata": {
        "id": "FDvFjkHSgH4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "def interpolation(lambda1, model, firstImage, secondImage):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # frist image latent vector\n",
        "        firstImage = firstImage.to(device)\n",
        "        latent_1, _ = model.encoder(firstImage)\n",
        "\n",
        "        # second image latent vector\n",
        "        secondImage = secondImage.to(device)\n",
        "        latent_2, _ = model.encoder(secondImage)\n",
        "\n",
        "        # interpolation of the two latent vectors\n",
        "        both_Img_Latent = lambda1* latent_1 + (1- lambda1) * latent_2\n",
        "\n",
        "        # reconstruction of the interpolated image\n",
        "        both_Img_Latent = model.decoder(both_Img_Latent)\n",
        "        both_Img_Latent = both_Img_Latent.cpu()\n",
        "\n",
        "        return both_Img_Latent\n",
        "    \n",
        "# Output part of the test dataset by number\n",
        "digits = [[] for _ in range(10)]\n",
        "for img_batch, label_batch in data_Loader:\n",
        "    for imgCount in range(img_batch.size(0)):\n",
        "        digits[label_batch[imgCount]].append(img_batch[imgCount:imgCount+1])\n",
        "    if sum(len(d) for d in digits) >= 1000:\n",
        "        break;\n",
        "\n",
        "# lambda interpolation\n",
        "lambda_range=np.linspace(0,1,10)\n",
        "\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for ind,l in enumerate(lambda_range):\n",
        "    both_Img_Latent=interpolation(float(l), vae, digits[7][0], digits[1][0])\n",
        "   \n",
        "    both_Img_Latent = to_img(both_Img_Latent)\n",
        "    \n",
        "    image = both_Img_Latent.numpy()\n",
        "   \n",
        "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
        "    axs[ind].set_title('lambda='+str(round(l,1)))\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "42AyreGlx8P5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}